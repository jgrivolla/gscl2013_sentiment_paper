\documentclass{llncs}

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
%\usepackage{amsmath}
\usepackage{graphicx}

\sloppy

\title{Sentiment Analysis and Visualization using UIMA and Solr}
\author{Carlos Rodríguez Penagos, David García Narbona, Guillem Massó Sanabre, Jens Grivolla, Joan Codina Filbà}
\institute{Barcelona Media Innovation Centre}

\begin{document}
\maketitle

\begin{abstract}
In this paper we present an overview of a UIMA-based system for Sentiment Analysis in hotel customer reviews. It extracts object-opinion/attribute-polarity triples using a variety of UIMA modules, some of which are adapted from freely available open source components and others developed fully in-house.

A Solr based graphical interface is used to explore and visualize the collection of reviews and the opinions expressed in them.
\end{abstract}

\section{Introduction}

With the continuing growth of Social Media such as Twitter, Facebook, and many others, both in terms of volume of content produced daily by users, and in terms of the impact it can have for reputation and decision making (buying, travelling, ...) there is a strong commercial need (and social interest) to efficiently analyze those vast amounts of mostly unstructured information and extract summarized knowledge, while also being able to explore and navigate the content.

We present here a prototype system for analyzing customer reviews of hotels, detecting what people talk about and what opinions they express. The processing at the level of individual reviews is done using UIMA with a variety of analysis engines, whereas the summary of the results, visualization and exploration interface is based on Solr.

\section{Extraction of Opinionated Units}

The prototype presented here focusses on extraction of customer opinions from full-text unstructured reviews, provided by the users of a big customer review site. We identified as the object of interest for our analysis what we call "opinionated units". These OUs consist in:
\begin{itemize}
\item the object of the opinion, i.e. the thing that is being commented on, which we call \emph{target}.
\item the opinion expression, i.e. the words or sentence fragments that represent what is being said about the target, which we call \emph{cues}.
\item the \emph{polarity} of the opinion, as it relates to the target.
\end{itemize}

Our system proceeds by first detecting possible opinion targets and possible cues in the review text. These target and cue candidates are then correlated to form opinionated units using relevant paths of the  syntactic dependencies graph that link the two together. Finally, the polarity of the opinionated unit is established using an apriori polarity taken from the cue (possibly dependent on the type of target) and taking into account quantifiers and negations that appear in the context of the opinionated unit.

\subsection{Target and Cue Detection}

For detection of the possible targets and cues that anchor the OUs, we relied on a JNET annotator that uses Conditional Random Fields over richly annotated vectors (POS, NER, polar words, etc.), and trained with a manually annotated corpus of similar customer reviews. 
On evaluating this process, we allowed for partial overlap (e.g. "The room" and "room" counted as equally correct answers), and we obtained models that had F1 of 0.69 for  Targets only, 0.54 for Cues and a combined Target/Cue identification model that provided an F1 of 0.62, with a top precision of 0.84 for the Cue only model and a top coverage (or recall) of 0.63 for Target-only models.


\subsection{Forming Opinionated Units}

In order to detect candidate opinion-bearing linguistic structures we parsed the sentences with the DeSR dependency parser trained with the Ancora Spanish corpus, modified with our own Parole-like tagset.We looked for possible paths in the graph linking Cues to Targets, as show in Figure  \ref{fig:GRAFO}, where the target "la habitación" (the room) is qualified by "pequeña" (small) and the Target "el desayuno" (breakfast) is being described as "with room for improvement".

\begin{figure}[ht]
\centering
\includegraphics[width=12cm]{grafo.pdf}
\caption{Opinionated Units in the dependency graph}
\label{fig:GRAFO}
\end{figure}

We identified both correct and wrong paths between targets and cues in annotated documents and analysed them. This allowed us to extract relevant patterns. The most important patterns are structures of linking verbs and name-adjective relations, but there are prepositional phrases, adverbial phrases and subject-verb relations as well. These structures can also be combined, such as a prepositional phrase in the subject of a linking verb (e.g. “the size of the room is good”). All the relevant paths can be represented by a limited number or regular expressions, which are used to correlate targets and cues of the same OU.

\subsection{Assigning Polarity}

We have been using different strategies  and tools to detect the polarity of an Opinionated Unit. A first strategy is to assign polarities to cues, and then expand this polarity to the opinionated unit, using an aproach based on the words sequence (Conditional Random Fields). A second strategy used support vector machines on a "bag of features" that included words, polar words and their polarity, negations, and quantifiers to build a feature vector used for training and classification. 

After statistical models have been applied, we also used heuristics that combined those polarities with the polarities of polar words (detected using dictionaries) in the context of the OUs, in order to assign a final polarity to the Opinionated Unit.

\begin{figure}[ht]
\centering
\includegraphics[width=9cm]{bookingOU.png}
\caption{Type representation of the Opinionated Unit visualized with the Annotation Viewer}
\label{fig:xmi}
\end{figure}

\section{Architecture and Implementation}

\subsection{Architecture Overview}

This prototype is designed to work on a static document collection, previously loaded into a MySQL database (including the review text as well as associated metadata). The overall architecture is shown in figure \ref{fig:arch}.

\begin{figure}[ht]
\centering
\includegraphics[width=9cm]{arch.pdf}
\caption{Architecture overview}
\label{fig:arch}
\end{figure}

%\subsection{Pipeline Overview}
%Figure \ref{fig:pipeline} shows an overview of the UIMA pipeline used for the prototype.
%\emph{AE sequence (sent,tok,pos,lemma,jnet,dicts,desr,OUs)...}

\subsection{UIMA modules}

This section describes all UIMA modules used in the prototype. Some of them are existing open source components, some are adaptations, and some are our own custom developments. We have started publishing our work on Github and will continue doing so as far as possible (see \url{https://github.com/BarcelonaMedia-ViL/}).

\subsubsection{UIMA Collection Tools}

UIMA Collection Tools is an \emph{ecosystem} of tools for allowing UIMA pipelines to store and retrieve data from database systems, such as MySQL.
Plain text documents can be retrieved from a database, XMI documents can be retrieved from and stored in a database either compressed or uncompressed, features can be extracted into a database table, and annotations within database-stored XMI blobs can be visualized the same way as the standard AnnotationViewer does for XMI files.

The UIMA Collection tools have been developed at Barcelona Media, some of them based on the example Collection Readers and CAS Consumers provided with the UIMA distribution. They are published under the Apache License at \url{https://github.com/BarcelonaMedia-ViL/uima-collection-tools}.

\begin{itemize}
\item \emph{DBCollectionReader} is a UIMA collection reader which retrieves plain text documents stored in a MySQL database. Database connection parameters as well as SQL query have to be specified in the component descriptor. It is derived from the FileSystemCollectionReader.

\item \emph{SolrCollectionReader} is equivalent to DBCollectionReader, but using a Solr index as the document source.

\item \emph{DBXMICollectionReader} is a UIMA collection reader which retrieves XMI documents stored in a MySQL database. Database connection parameters as well as SQL query have to be specified in the component descriptor. DBXMICollectionReader is also prepared to read compressed XMI documents by means of ZLIB compression. This option can be set in the descriptor file.

\item \emph{DBAnnotationsCASConsumer} is a CAS consumer which stores in a  MySQL database table the values of the features specified in the component descriptor file. Each table row corresponds to the annotation defined as the \emph{splitting annotation}, e.g. if Sentence annotation has been defined as the \emph{splitting annotation}, each table row will correspond to a Sentence, and this row will contain features of the Sentence annotation and/or features of annotations covered by the Sentence annotation.

\item \emph{DBXMICASConsumer} is a CAS consumer that persists XMI documents in a database. Database connection parameters as well as necessary parameters for data storage have to be specified in the component descriptor (database table name, and both table fields for XMI document id and XMI document).
DBXMICASConsumer is also prepared to store compressed XMI documents by means of ZLIB compression. This option can be set in the descriptor file.

\item \emph{DBAnnotationViewer} is a modification of the Annotation Viewer UIMA tool. It allows to read XMI files from a database (currently only works with MySQL databases) and to view their annotations the same way Annotation Viewer does.
This tool is aimed to be used along with DBXMICollectionReader and DBXMICASConsumer. Both Collection Reader and CAS Consumer manage XMI files from and to databases. DB Annotation Viewer enables to view UIMA annotations within these XMI files stored in a database, without neededing to extract these XMI files from the database.
\end{itemize}

\subsubsection{OpenNLP}

We use OpenNLP\footnote{\url{http://opennlp.apache.org/}} with the standard UIMA wrappers for our base pipeline, including Sentence Detector, Tokenizer, and POS Tagger. The components are used without any software modifications, using our own trained models for Spanish.

\subsubsection{Lemmatizer}

We apply Lemmatization using a large form dictionary developed in-house. Candidate lemmas are first added to the CAS using ConceptMapper\footnote{\url{http://uima.apache.org/sandbox.html\#concept.mapper.annotator}} and then filtered by POS tag using a custom component.

\subsubsection{JNET}

For statistical detection of targets and cues we use JNET\footnote{\url{http://www.julielab.de/Resources/Software/NLP\_Tools.html}} (the Julielab Named Entity Tagger), which is based on Conditional Random Fields (CRF).

It detects token sequences that belong to certain classes, taking into account  a variety of features associated with each token (such as the surface form, lemma, POS tag, surface features such as capitalization, etc.) as well as its context of preceeding and succeeding tokens. While originally intended for Named Entity Recognition, we trained JNET with our own manually annotated corpus of targets and cues.

Compared to the original JNET as released by Julielab we introduced a series of changes, most importantly making it type system independent by taking all input and output types and features as parameters, and fixing some bugs that were triggered when using a larger amount of token features.

We expect to release our changes soon, but are still looking into the question of licensing, to comply with JNET's original license.

\subsubsection{DeSR}

We developed a UIMA wrapper for the DeSR dependency parser\footnote{\url{https://sites.google.com/site/desrparser/}}. The parser creates dependency annotations based on previously generated sentence, token and POStag annotations. It is available at \url{https://github.com/BarcelonaMedia-ViL/desr-uima}.

The UIMA DeSR analysis engine is a UIMA C++ annotator, developed using the C++ SDK provided by UIMA. It translates between the format required by the DeSR parser shared library and the UIMA CAS format. The mapping between UIMA types and features and the features used internally by DeSR is configurable in the annotator descriptor.

\subsubsection{DependencyTreeWalker}

This is a Pythonnator-based analysis engine for wrapping DependencyGraph Python module (both developed in-house). This allows us to work easily with the dependency graph generated by DeSR in order to e.g. determine and validate the path between two given UIMA annotations.

\subsubsection{Weka Wrapper}

We used the Mayo Weka/UIMA Integration (MAWUI\footnote{\url{http://informatics.mayo.edu/text/index.php?page=weka}}), as a basis for the machine learning tools. The version we use is adapted to newer versions of UIMA and made much more configurable. MAWUI generates a single vector for each document, that is used to classify it as a whole. In our case, a document can contain several Opinionated Units that need to be classified. For this reason the Weka Wrapper was adapted to be able to deal with all the annotations of a given type inside a document (or collection when generating the training data).

\section{Visualization}

Beyond being able to extract and classify the opinions, users need an interface that allows them to access and explore the data. They need to know which are the targets or aspects that are being addressed by the opinions and what is being said about them, and this has to be shown in an aggregated way, with drill down capabilities, so that the end user has a clear view of the contents of hundreds or thousands of opinions.

UIMA does not provide tools to deal with collections of documents, and we use Solr, a Lucene based indexing tool, to index the Opinionated Units. Through the use of Solr's faceting and pivot utilities we are able to graphically summarize thousands of opinions. Special charts have been done in order to allow not only to represent the data but also to select subsets of opinions and summarize and compare them. For example, we can compare the global user's opinions with the opinions about a single hotels or the hotels in a specific area. 

To index the data we needed the linguistic information, but also the metadata associated with the opinion, which is located in databases and is not processed with UIMA. For this reason we import the data to Solr in two steps. In a first step we generate from UIMA a table with the data that we then import to Solr together with the metadata.

\subsection{Indexing Opinionated Units}

To index the Opinionated Units we use the DBAnnotationsCASConsumer component. We generate a register for each opinionated unit, containing: the target, the cue, the text span, the polar words, their polarity, the polarity of the cue, and the polarity of the Opinionated Unit. Cues and targets are grouped in single tokens by means of underscores.

We use the the DataImportHandler from Solr in order to import the data from the database. To do it, a query combines the opinionated unit information with the one related with the hotel or the user who writes the opinion. Cues are indexed twice, once all merged and secondly in different fields depending on the opinion's polarity, making it easy to retrieve just the positive or negative opinion markers.

We took this option because it is a bit faster, more flexible and reliable than the other ones: when indexing directly from UIMA we have problems to add all the desired metadata, and if we call UIMA from Solr (or Lucene) then it is difficult to have a general framework that splits a single document into several Opinionated Units.

Solr, which is based on Lucece, is an inverted index for text, and it also includes some facilities to apply basic text processing (synonyms, stop words removal or stemming). This treatment is done at index time, allowing for very fast queries.
  
\subsection{Ajax-Solr}

AJAX-Solr (https://github.com/evolvingweb/ajax-solr) is a JavaScript library for creating user interfaces to Apache Solr. This library works with facets. Faceting is a capability of Solr that allows to have a fast statistic of the most frequent terms in each field, after performing a query. Since version 4.0 Solr also has pivots that combine the facets from two or more different fields. We adapted AJAX-Solr to work with pivots and wrote a series of widgets to visualize them. Our own extensions to AJAX-Solr are also published on github\footnote{\url{https://github.com/BarcelonaMedia-ViL/ajax-solr}}.

By means of clicking the different facets that appear on the widgets, the user can build a query that restricts the set of opinions to summarize. These opinions are then summarized by showing the most frequent terms they contain, or the most differentiating ones\footnote{the most differentiating ones are those terms that are frequent in the current subset but that are less frequent in the general one}. 

Figure \ref{fig:solr} shows the pivot result in text and force diagram format. It shows the relationship between targets and positive and negative cues. In the textual representation, the relationships are not shown directly but scaled to magnify the most differentiating ones. 

\begin{figure}[ht]
\centering
\includegraphics[width=9cm]{solr_visualization.png}
\caption{Visualization of Cue and Target correlations across the whole corpus} 
\label{fig:solr}
\end{figure}

\section{Conclusion}
The combination of UIMA and Solr has allowed us to to develop a very flexible platform that makes it easy to integrate and combine processing modules from a variety of sources and in a variety of programming languages, as well as navigate and visualize the results easily and efficiently.

We found many useful UIMA components to be available as open source, and encountered few compatibility issues (other than adapting some components to be type system independent). Solr provides us with a very flexible platform to access large document collections, and in combination with UIMA allows us to explore even complex hidden relationships within those collections.

We found the different open source communities to be very receptive, and try to participate by publishing our own contributions under permissive licenses that make them easy for others to adopt and use.

\section{Thanks}
This work has been partially funded by the Spanish
Government project Holopedia, TIN2010-21128-
C02-02, and the CENIT program project Social Media,
CEN-20101037.

\end{document}